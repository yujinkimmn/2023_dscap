import streamlit as st
import altair as alt
import pandas as pd
import time
import re
from vega_datasets import data
from datetime import datetime
import folium
from folium.plugins import HeatMap, MarkerCluster
from streamlit_folium import st_folium
from streamlit_modal import Modal
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from wordcloud import WordCloud


fp1 = "/Users/yeon/Documents/2023/ds_capstone/2023_dscap/EDA/Region_analysis/ì§€ì—­ë³„ë§ˆì•½ë¥˜ë¹ˆë„_ìœ„ê²½ë„í¬í•¨_ìµœì¢….csv"
fp2 = "/Users/yeon/Documents/2023/ds_capstone/2023_dscap/twitterdata/labeling/total_labeling_preprocessed.csv"
fp3 = "/Users/yeon/Documents/2023/ds_capstone/2023_dscap/twitterdata/preprocessing/preprocessed/total_preprocessed_name_revise.csv"  #ì´ë¦„ íŠ¹ìˆ˜ê¸°í˜¸ ì²˜ë¦¬

file_timeseries = "./files/total_preprocessed.csv"

file_clustering = "./files/total_tokenized_mecab.csv"

@st.cache_data
def load_data(file_path):
    data = pd.read_csv(file_path, encoding = "utf-8")
    return data

@st.cache_data(ttl=60 * 60 * 24)
def get_timechart(data):
    hover = alt.selection_single(
        fields=["date"],
        nearest=True,
        on="mouseover",
        empty="none",
    )
    
    
    lines = (
        alt.Chart(data, height=500, title="ë§ˆì•½ ê±°ë˜ ê²Œì‹œë¬¼ ì¶”ì´")
        .mark_bar()
        .encode(
            x="date",
            y="count",
            color=alt.Color("count"),
        )
    )
    
    points = lines.transform_filter(hover).mark_circle(color="red", size=300)

    tooltips = (
        alt.Chart(data)
        .mark_rule()
        .encode(
            x="date",
            y="count",
            opacity=alt.condition(hover, alt.value(0.3), alt.value(0)),
            tooltip=[
                alt.Tooltip("date", title="Date"),
                alt.Tooltip("count", title="Count"),
            ],
        )
        .add_selection(hover)
    )

    return (lines + points + tooltips).interactive()
    
def timeseries(file_path):
    global df
    df = load_data(file_path)
    global flowday
    
    for i in range(len(df['date'])):
        df['date'][i] = str(df['date'][i]).split(" ")[0]
    
    df['year'] = 0
    df['month'] = 0
    df['day'] = 0

    for i in range(len(df['date'])):
      if (str(df['date'][i])[4] == '.') :
        df['date'][i] = str(df['date'][i].split('.')[0]) + "-" + str(df['date'][i].split('.')[1]) + "-" + str(df['date'][i].split('.')[2])
      else:
        df['date'][i] = str(df['date'][i].split('-')[0]) + "-" + str(df['date'][i].split('-')[1]) + "-" + str(df['date'][i].split('-')[2])


    df['date'] = pd.to_datetime(df['date'])

    print(df['date'][0])

    for i in range(len(df['date'])):
      df['year'][i] = str(df['date'][i]).split("-")[0]
      df['month'][i] = str(df['date'][i]).split("-")[1]
      df['day'][i] = (str(df['date'][i]).split("-")[2]).split(" ")[0]

    flowday = df.groupby(['year', 'month', 'day']).count().reset_index()
    flowday = flowday.iloc[0:, :4]
    flowday['date'] = 0
    
    for i in range(len(flowday)):
        flowday['date'][i] = str(flowday['year'][i]) + "-" + str(flowday['month'][i]) + "-" + str(flowday['day'][i])

    flowday['date'] = pd.to_datetime(flowday['date'])
    flowday = flowday.iloc[0:, 3:5]
    flowday = flowday.rename(columns={'type1':'count'})
    
    print(flowday)
    # ê¸°ë³¸ line chart í˜•íƒœ
    #st.line_chart(flowday, x="date", y="count")
    chart = get_timechart(flowday)
    
    ANNOTATIONS = [
    ("2021-11-03", "ì¼ 5ê°œ ì´ë‚´ ìœ ì§€"),
    ("2022-08-19", "ë§ˆì•½ ê±°ë˜ ê²Œì‹œë¬¼ ì¦ê°€ ì¶”ì„¸"),
    ("2022-12-25", "ë§ˆì•½ ê±°ë˜ ê²Œì‹œë¬¼ ê°ì†Œ ì¶”ì„¸"),
    ("2022-10-28", "ë§ˆì•½ ê±°ë˜ ê²Œì‹œë¬¼ ê¸‰ì¦"),
    ("2023-03-28", "ì¼ 1000ê°œ ëŒíŒŒ"),]
    
    annotations_df = pd.DataFrame(ANNOTATIONS, columns=["date", "event"])
    annotations_df.date = pd.to_datetime(annotations_df.date)
    annotations_df["y"] = 10
    
    annotation_layer = (
    alt.Chart(annotations_df)
    .mark_text(size=20, text="â¬‡", dx=-8, dy=-10, align="left")
    .encode(
        x="date:T",
        y=alt.Y("y:Q"),
        tooltip=["event"],
    )
    .interactive())

    st.altair_chart((chart + annotation_layer), use_container_width=True)
    
@st.cache_data
def load_data(file_path):
    data = pd.read_csv(file_path)
    return data
    

def visualize_clusters(df, n_clusters):
    graph = alt.Chart(df.reset_index()).mark_point(filled=True, size=60).encode(
        x=alt.X('x_values'),
        y=alt.Y('y_values'),
        color=alt.Color('labels'),
        tooltip=['word', 'labels']
    ).interactive()
    st.altair_chart(graph, use_container_width=True)
    

def clustering(file_path):
    global df
    df = load_data(file_path)
    
    print(df)
    
    for i in range(len(df['mecab'])):
        df['mecab'].loc[i] = eval(df['mecab'].loc[i])
        
    total = []
    for idx in range(len(df['mecab'])):
      word = []
      for i, j in df['mecab'].loc[idx]:
        if j == "NNP" or j == "SL" or j == "NNG":
          word.append(i)
      total.append(word)
    
    df = df.assign(word = total)
    
    corpus = []
    for i in df['word']:
      corpus.append(i)

    corpus_total = []
    for i in df['word']:
      for j in i:
        corpus_total.append(j)
    
    model = Word2Vec(corpus, vector_size=100, window = 4, min_count = 1, workers = 4, sg = 1)
    
    model_result = model.wv.most_similar('ì‘ëŒ€ê¸°')
    print(model_result)

    # fit a 2d PCA model to the vectors

    vectors = model.wv.vectors
    words = list(model.wv.key_to_index.keys())
    pca = PCA(n_components=2)
    PCA_result = pca.fit_transform(vectors)

    # prepare a dataframe
    words = pd.DataFrame(words)
    PCA_result = pd.DataFrame(PCA_result)
    PCA_result['x_values'] =PCA_result.iloc[0:, 0]
    PCA_result['y_values'] =PCA_result.iloc[0:, 1]
    PCA_final = pd.merge(words, PCA_result, left_index=True, right_index=True)
    PCA_final['word'] =PCA_final.iloc[0:, 0]
    PCA_data_complet =PCA_final[['word','x_values','y_values']]

    kmeans = KMeans(n_clusters=5)
    kmeans.fit(PCA_result.iloc[0:,:2])
    PCA_data_complet['labels'] = kmeans.predict(PCA_result.iloc[0:,:2])
    
    # ë‹¨ì–´ ë¹ˆë„ ìˆ˜ ì—´ ì¶”ê°€
    PCA_data_complet['counts'] = 0

    for i, word in enumerate(PCA_data_complet['word']):
      PCA_data_complet['counts'][i] = corpus_total.count(word)
    
    global PCA_drug
    PCA_drug = PCA_data_complet[PCA_data_complet['labels']==PCA_data_complet.loc[0].labels]
    
    kmeans = KMeans(n_clusters=2)
    kmeans.fit(PCA_drug.iloc[0:,1:3])
    PCA_drug['labels'] = kmeans.predict(PCA_drug.iloc[0:,1:3])
    
    print(PCA_drug)
    
    visualize_clusters(PCA_drug, 2)


def w_cloud():
    PCA_drug_removed = PCA_drug

    keywords = ['ìŠ¤í‹¸ë…¹ìŠ¤', 'ì‹ ì˜ëˆˆë¬¼', 'ì—í† ë¯¸ë°ì´íŠ¸', 'ì˜¥ì‹œì½”ëˆ', 'ì¡¸í”¼ë€', 'íŠ¸ë¼ë§ˆëŒ', 'ìº”ë””ì¼€ì´', 'ì¼€íƒ€ë¯¼',
                'ëŸ¬ì‰¬íŒŒí¼', 'ëìŠˆ', 'ì •ê¸€ì£¼ìŠ¤', 'ì—˜ì—ìŠ¤ë””', 'ì—‘ìŠ¤í„°ì‹œ', 'ë§ˆë²•ì˜ë²„ì„¯', 'í™˜ê°ë²„ì„¯', 
                'ë–¨ì•¡', 'ë¶í•œì‚°ì•„ì´ìŠ¤', 'ë¹™ë‘', 'ì‚¥ë‘', 'ì‚¬ë¼', 'ìƒ¤ë¶€', 'ì‹œì›í•œìˆ ', 'ì•„ì´ìŠ¤ìˆ ', 'ì•¡ìƒë–¨', 'ì‘ëŒ€ê¸°',
                'íˆë¡œë½•', 'í¬ë¦¬ìŠ¤íƒˆ', 'ì°¨ê°€ìš´ìˆ ', 'ì•„ì´ìŠ¤', 'ì°¬ìˆ ', 'ë“œë¼í¼', 'ë¸Œì•¡', 'ì•„ì´ìŠ¤ë“œë', 'í´ëŸ½ì•½', 
                'í…”ë ˆ', 'íŒŒí‹°ì•½', 'íŒ¨ì¹˜', 'í›„ë¦¬ë² ì´ìŠ¤', 'ì£¼ì‚¬ê¸°', 'í—ˆë¸Œ', 'ë¬¼ë½•', 'ë°œì •ì œ', 'ìµœìŒì œ', 'ì‚¬í‹°ë°”',
                'ì¸ë””ì¹´', 'í•©ì„±ëŒ€ë§ˆ', 'í•´ì‹œì‹œ', 'ëŒ€ë§ˆì´ˆ']

    for i in keywords:
        PCA_drug_removed = PCA_drug_removed[~PCA_drug_removed['word'].str.contains(i)]  #í‚¤ì›Œë“œ ì œì™¸í•œ ê²°ê³¼

    words_rev = []
    counts_rev = []
    words_list_rev = list(PCA_drug_removed['word'])
    counts_list_rev = list(PCA_drug_removed['counts'])

    for i in range(len(words_list_rev)):
        if len(words_list_rev[i]) >= 2:  #2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
            words_rev.append(words_list_rev[i])
            counts_rev.append(counts_list_rev[i])
    word_count_removed = dict(zip(words_rev, counts_rev))    

    #ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¬ê¸°
    wordcloud = WordCloud(font_path = 'NanumBarunGothic', background_color = 'white', colormap = 'rainbow_r',
                     width = 4000, height = 3000).generate_from_frequencies(word_count_removed)
    fig = plt.figure()
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()
    st.pyplot(fig)   



def histogram():
    global fp3
    df = pd.read_csv(fp3)

    df.drop_duplicates(['content', 'user.username', 'date'], inplace=True)  #ì¤‘ë³µìœ¼ë¡œ ìˆ˜ì§‘ëœ íŠ¸ìœ— ì œê±°
    df_grouped = pd.DataFrame(df.groupby('user.displayname')['content'].count())  #ë‹‰ë„¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê°œìˆ˜ ì„¸ê¸°
    df_grouped.reset_index(inplace = True)
    df_grouped.sort_values('content', inplace=True, ascending=False)
    df_new = df_grouped[df_grouped['content'] >= 20]  #20ë²ˆ ì´ìƒ ë“±ì¥í•œ ë‹‰ë„¤ì„

    #íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize = (20,15))
    sns.set(font = 'NanumBarunGothic', font_scale = 1.5, rc = {'axes.unicode_minus': False}, style = 'darkgrid')
    fig = sns.barplot(
        x = 'content', y = 'user.displayname', data = df_new, width = 0.8)
    fig.set_yticks(np.arange(0, len(df_new)+1, 2))
    fig.set_title('User Name Appeared More Than 20 Times', fontsize = 20)
    fig.set_xlabel('Frequency', size = 15)
    fig.set_ylabel('Name', size = 15)
    fig = fig.figure
    st.pyplot(fig)

    
def popup_html():
    html = '''<!DOCTYPE html>
<html>
<head>
  <title>Twitter Design</title>
  <style>
    /* CSS styles */
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 20px;
    }
    
    .tweet {
      margin-bottom: 20px;
      border: 1px solid #ccc;
      padding: 10px;
      background-color: #f9f9f9;
    }
    
    .username {
      font-weight: bold;
      color: #1da1f2;
    }
    
    .content {
      margin-top: 5px;
    }
    
    .userid {
      font-size: 0.8em;
      color: #777;
    }
    /* Custom modal size */
    .custom-modal {
        max-width: 800px;
        height: 400px;
        overflow-y: scroll;
        scrollbar-width: thin;
    }
  </style>
</head>
<body>
  <div class="tweet">
    <div class="username">JohnDoe</div>
    <div class="userid">@johndoe123</div>
    <div class="content">
      Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis gravida lobortis aliquet.
    </div>
  </div>
  
  <div class="tweet">
    <div class="username">JaneSmith</div>
    <div class="userid">@janesmith456</div>
    <div class="content">
      Nulla nec ex vitae nisi feugiat rhoncus id non neque.
    </div>
  </div>
</body>
</html>
'''

    return html


def map_visualization():
    global fp1
    data = load_data(fp1)

    location = [[lat, lng] for lat, lng, region in zip(data['ìœ„ë„'], data['ê²½ë„'], data['ì§€ì—­ëª…']) if region == region_option]
    lat = location[0][0]
    lng = location[0][1]
    zoom = 9
    if region_option == "ì„œìš¸íŠ¹ë³„ì‹œ":
        zoom = 12

    m = folium.Map(location=[lat, lng], zoom_start=zoom, tiles='OpenStreetMap')
    # Convert data to proper types
    lats = data['ìœ„ë„'].values.astype(float)
    lngs = data['ê²½ë„'].values.astype(float)
    freqs = data['ë¹ˆë„'].values.astype(float)
    # Create a HeatMap layer
    heat_data = [[lat, lng, freq] for lat, lng, freq in zip(lats, lngs, freqs)]
    HeatMap(heat_data).add_to(m)

    # Create a MarkerCluster layer
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the MarkerCluster layer
    for index, row in data.iterrows():
        lat = row['ìœ„ë„']
        lng = row['ê²½ë„']
        freq = row['ë¹ˆë„']
        drug = row['ë§ˆì•½ë¥˜']
        popup_content = folium.Popup(f'ë§ˆì•½ë¥˜: {drug}', max_width=300)

        # Create a marker and add it to the MarkerCluster layer
        marker = folium.Marker(location=[lat, lng], popup=popup_content, tooltip='ë§ˆì•½ë¥˜ í™•ì¸')
        marker.add_to(marker_cluster)

    st_data = st_folium(m, width=725)


# Set up the layout
st.set_page_config(page_title="Dashboard", page_icon="ğŸ“Š", 
                   layout='wide', initial_sidebar_state='expanded')

with st.sidebar:
    with st.spinner("Loading..."):
        time.sleep(0.5)
    
    st.subheader("Parameter")
    # í•´ë‹¹ ê¸°ê°„ì˜ ì‹œê³„ì—´ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ì‹¶ì—ˆìŒ! ë¹¼ë„ ìƒê´€ì—†ì–´ìœ ~
    time_range = st.slider("ë‚ ì§œë¥¼ ì„¤ì •í•˜ì‹œì˜¤", 
                           value=(datetime(2021, 1, 1), datetime(2023, 3, 31)),
                           format="YYYY/MM/DD")
    
    region_option = st.selectbox("ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”", 
                          ("ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ", "ëŒ€êµ¬ê´‘ì—­ì‹œ", "ì¸ì²œê´‘ì—­ì‹œ", "ê´‘ì£¼ê´‘ì—­ì‹œ",
                           "ëŒ€ì „ê´‘ì—­ì‹œ", "ìš¸ì‚°ê´‘ì—­ì‹œ", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "ê²½ê¸°ë„", "ê°•ì›ë„", 
                           "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë‚¨ë„", "ì „ë¼ë¶ë„", "ì „ë¼ë‚¨ë„", "ê²½ìƒë¶ë„", "ê²½ìƒë‚¨ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„"))




    
    
col1, col2 = st.columns([3,2])
with col1:
    st.title("Map")
    map_visualization()

    
with col2:
    st.title("Series")
    # Add code
    flowday = pd.DataFrame()
    timeseries(file_timeseries)


    tab1, tab2, tab3= st.tabs(['Clustering' , 'Wordcloud', 'Histogram'])
    
    with tab1:
        st.subheader("Clustering")
        clustering(file_clustering)
        
    
    with tab2: 
        st.subheader("Wordcloud")
        w_cloud()

    with tab3:
        st.subheader("Histogram")
        histogram()
